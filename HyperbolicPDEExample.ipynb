{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76156cf2",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Make sure `PDEControlGym` is correctly installed according to [doc](https://pdecontrolgym.readthedocs.io/en/latest/guide/install.html). This file is modified based on `PDEControlGym/examples/transportPDE/HyperbolicPDEExample.ipynb` from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8ab6d-b602-496c-968e-948d3d5dbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pde_control_gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import stable_baselines3\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307a8313-126e-479f-b94c-69dbf2188e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version 0.29.1\n",
      "Numpy version 1.26.2\n",
      "Stable Baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Print Versioning\n",
    "print(\"Gym version\", gym.__version__)\n",
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"Stable Baselines3 version\", stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32177259-8979-4957-baf5-e8f15c9c563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NOISE\n",
    "def noiseFunc(state):\n",
    "    return state\n",
    "\n",
    "# Chebyshev Polynomial Beta Functions\n",
    "def solveBetaFunction(x, gamma):\n",
    "    beta = np.zeros(len(x), dtype=np.float32)\n",
    "    for idx, val in enumerate(x):\n",
    "        beta[idx] = 5*math.cos(gamma*math.acos(val))\n",
    "    return beta\n",
    "\n",
    "# Kernel function solver for backstepping\n",
    "def solveKernelFunction(theta):\n",
    "    kappa = np.zeros(len(theta))\n",
    "    for i in range(0, len(theta)):\n",
    "        kernelIntegral = 0\n",
    "        for j in range(0, i):\n",
    "            kernelIntegral += (kappa[i-j]*theta[j])*dx\n",
    "        kappa[i] = kernelIntegral  - theta[i]\n",
    "    return np.flip(kappa)\n",
    "\n",
    "# Control convolution solver\n",
    "def solveControl(kernel, u):\n",
    "    res = 0\n",
    "    for i in range(len(u)):\n",
    "        res += kernel[i]*u[i]\n",
    "    return res*1e-2\n",
    "\n",
    "# Set initial condition function here\n",
    "def getInitialCondition(nx):\n",
    "    return np.ones(nx)*np.random.uniform(1, 10)\n",
    "\n",
    "# Returns beta functions passed into PDE environment. Currently gamma is always\n",
    "# set to 7.35, but this can be modified for further problesms\n",
    "def getBetaFunction(nx):\n",
    "    return solveBetaFunction(np.linspace(0, 1, nx), 7.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1f129a-e88d-48d1-96ba-da0c85abda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep and spatial step for PDE Solver\n",
    "T = 5\n",
    "dt = 1e-4 \n",
    "dx = 1e-2\n",
    "X = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9fc8b-2b8f-4e1b-a46b-64222b995b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backstepping does not need to normalize actions to be between -1 and 1, so normalize is set to False. Otherwise, \n",
    "# parameters are same as RL algorithms\n",
    "from pde_control_gym.src import TunedReward1D,NormReward\n",
    "reward_class =  TunedReward1D(int(round(T/dt)), -1e3, 3e2) # with penalize\n",
    "hyperbolicParameters = {\n",
    "        \"T\": T, \n",
    "        \"dt\": dt, \n",
    "        \"X\": X,\n",
    "        \"dx\": dx, \n",
    "        \"reward_class\": reward_class,\n",
    "        \"normalize\":None, \n",
    "        \"sensing_loc\": \"full\", \n",
    "        \"control_type\": \"Dirchilet\", \n",
    "        \"sensing_type\": None,\n",
    "        \"sensing_noise_func\": lambda state: state,\n",
    "        \"limit_pde_state_size\": True,\n",
    "        \"max_state_value\": 1e10,\n",
    "        \"max_control_value\": 20,\n",
    "        \"reset_init_condition_func\": getInitialCondition,\n",
    "        \"reset_recirculation_func\": getBetaFunction,\n",
    "        \"control_sample_rate\": 0.1\n",
    "}\n",
    "\n",
    "hyperbolicParametersBackstepping = hyperbolicParameters.copy()\n",
    "hyperbolicParametersBackstepping[\"normalize\"] = False\n",
    "\n",
    "hyperbolicParametersRL = hyperbolicParameters.copy()\n",
    "hyperbolicParametersRL[\"normalize\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124e4f5",
   "metadata": {},
   "source": [
    "Relace `PPO_MODEL_PATH` and `SAC_MODEL_PATH` with the pretrained PPO and SAC nominal controller from `PDEControlGym`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bd8f0-7f37-404d-bb6c-f857f0fd3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoModelPath = \"PPO_MODEL_PATH\"\n",
    "sacModelPath = \"SAC_MODEL_PATH\"\n",
    "\n",
    "ppoModel = PPO.load(ppoModelPath)\n",
    "sacModel = SAC.load(sacModelPath)\n",
    "\n",
    "# For backstepping controller\n",
    "spatial = np.linspace(dx, X, int(round(X/dx)))\n",
    "beta = solveBetaFunction(spatial, 7.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c93c80f-723e-4fc0-98a5-deeb0db023ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a single epsiode calculation\n",
    "# Parameter varies. For SAC and PPO it is the model itself\n",
    "# For backstepping it is the beta function\n",
    "def runSingleEpisode(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter)\n",
    "        \n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "        # print(action, obs)\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "    u = np.array(uStorage)\n",
    "    return rew, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49278e04-7771-4a5c-9d75-0ca82a512b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Controllers\n",
    "def bcksController(obs, beta):\n",
    "    kernel = solveKernelFunction(beta)\n",
    "    return solveControl(kernel, obs)\n",
    "\n",
    "def RLController(obs, model):\n",
    "    action, _state = model.predict(obs)\n",
    "    return action\n",
    "\n",
    "def openLoopController(_, _a):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55870e9",
   "metadata": {},
   "source": [
    "Collect data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9473d-b1c4-49f8-a805-e697760e0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect dataset\n",
    "import scipy\n",
    "# from tqdm import tqdm\n",
    "from tqdm import trange, tqdm\n",
    "def getInitialConditionRandom(nx):\n",
    "    return np.ones(nx) * (1 + np.random.rand() * 9)\n",
    "\n",
    "\n",
    "hyperbolicParametersRL[\"reward_class\"] = TunedReward1D(int(round(T/dt)), -1e3, 3e2)\n",
    "hyperbolicParametersBackstepping[\"reward_class\"] = TunedReward1D(int(round(T/dt)), -1e3, 3e2)\n",
    "\n",
    "hyperbolicParametersBacksteppingRandom = hyperbolicParametersBackstepping.copy()\n",
    "hyperbolicParametersBacksteppingRandom[\"reset_init_condition_func\"] = getInitialConditionRandom\n",
    "\n",
    "\n",
    "\n",
    "hyperbolicParametersRLRandom = hyperbolicParametersRL.copy()\n",
    "hyperbolicParametersRLRandom[\"reset_init_condition_func\"] = getInitialConditionRandom\n",
    "\n",
    "\n",
    "# Make environments\n",
    "envBcksRandom = gym.make(\"PDEControlGym-TransportPDE1D\", **hyperbolicParametersBacksteppingRandom)\n",
    "\n",
    "\n",
    "envRLRandom = gym.make(\"PDEControlGym-TransportPDE1D\", **hyperbolicParametersRLRandom)\n",
    "\n",
    "xs_bcks = []\n",
    "ys_bcks = []\n",
    "xs_ppo = []\n",
    "ys_ppo = []\n",
    "xs_sac = []\n",
    "ys_sac = []\n",
    "for i in range(50000):\n",
    "    rewBcksRandom, uBcksRandom = runSingleEpisode(bcksController, envBcksRandom, beta)\n",
    "    xs_bcks.append((uBcksRandom.transpose())[-1])\n",
    "    ys_bcks.append((uBcksRandom.transpose())[0])\n",
    "\n",
    "    rewPPORandom, uPPORandom = runSingleEpisode(RLController, envRLRandom, ppoModel)\n",
    "    xs_ppo.append((uPPORandom.transpose())[-1])\n",
    "    ys_ppo.append((uPPORandom.transpose())[0])\n",
    "    \n",
    "\n",
    "    rewSACRandom, uSACRandom = runSingleEpisode(RLController, envRLRandom, sacModel)\n",
    "    xs_sac.append((uSACRandom.transpose())[-1])\n",
    "    ys_sac.append((uSACRandom.transpose())[0])\n",
    "    print(rewPPORandom,rewSACRandom)\n",
    "    print((uSACRandom)[-1])\n",
    "    # if i % 1000 == 0: print(i)\n",
    "    \n",
    "data_bcks = {\"a\": np.stack(xs_bcks), \"u\": np.stack(ys_bcks)}\n",
    "scipy.io.savemat(\"data_bcks_hyperbolic.mat\", data_bcks)\n",
    "\n",
    "data_ppo = {\"a\": np.stack(xs_ppo), \"u\": np.stack(ys_ppo)}\n",
    "scipy.io.savemat(\"data_ppo_hyperbolic.mat\", data_ppo)\n",
    "\n",
    "data_sac = {\"a\": np.stack(xs_sac), \"u\": np.stack(ys_sac)}\n",
    "scipy.io.savemat(\"data_sac_hyperbolic.mat\", data_sac)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11658a",
   "metadata": {},
   "source": [
    "# Safety filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSingleEpisodeQP(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter,i)\n",
    "        # print(action)\n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "        # print(action, obs)\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "        i += 1\n",
    "    u = np.array(uStorage)\n",
    "    return rew, u\n",
    "\n",
    "def QP_filter_Controller(obs, parameter,index):\n",
    "    # print(obs)\n",
    "    # print(parameter)\n",
    "    return parameter[index+1]\n",
    "\n",
    "def find_earliest_true(condition):\n",
    "    # Iterate over the first two dimensions (10 and 8) and check for each slice\n",
    "    earliest_indices = np.full(condition.shape[:2], 0)  # Initialize with -1 (indicating no valid index)\n",
    "\n",
    "    for i in range(condition.shape[0]):  # Iterate over first dimension\n",
    "        for j in range(condition.shape[1]):  # Iterate over second dimension\n",
    "            # For each slice (i, j), find the earliest index where the condition is True\n",
    "            # and all subsequent values are also True\n",
    "            for k in range(condition.shape[2]):\n",
    "                if not condition[i, j, condition.shape[2]-k-1]: \n",
    "                    # print(k)\n",
    "                    if k == 0:\n",
    "                        earliest_indices[i,j] = -1\n",
    "                    else:\n",
    "                        earliest_indices[i,j] = condition.shape[2]-k\n",
    "                    break\n",
    "    return earliest_indices\n",
    "\n",
    "reward_class_no_penalty = TunedReward1D(int(round(T/dt)), -1e-4, 3e2) # no penalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3a8f9",
   "metadata": {},
   "source": [
    "Replace the filtered results `FILTER_RESULT_PATH` with the one saved in  `test_cbf_hyper.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdaba5-9c29-4ae2-8a7f-8c5783a271d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result_path = \"FILTER_RESULT_PATH\"\n",
    "RL_1000 = np.load(filtered_result_path)\n",
    "RL_reward_beforeQP = []\n",
    "RL_reward_afterQP = []\n",
    "uBcks_beforeQP_list = []\n",
    "uBcks_afterQP_list = []\n",
    "for i in range(RL_1000[\"safe_label\"].transpose().shape[0]):\n",
    "    U_list = RL_1000[\"U_nominal\"][:, i]\n",
    "\n",
    "    def getInitialConditionFixed(nx):\n",
    "        return np.ones(nx) * U_list[0]\n",
    "    hyperbolicParametersBacksteppingFixed = hyperbolicParametersBackstepping.copy()\n",
    "    hyperbolicParametersBacksteppingFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    hyperbolicParametersBacksteppingFixed[\"reward_class\"] = reward_class_no_penalty\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-TransportPDE1D\", **hyperbolicParametersBacksteppingFixed)\n",
    "    reward_beforeQP, uBcks_beforeQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_list)\n",
    "    uBcks_beforeQP_list.append(uBcks_beforeQP)\n",
    "    RL_reward_beforeQP.append(reward_beforeQP)\n",
    "\n",
    "    U_safe_list = RL_1000[\"U_safe\"][:, i]\n",
    "    def getInitialConditionFixed(nx):\n",
    "        return np.ones(nx) * U_list[0]\n",
    "    hyperbolicParametersBacksteppingFixed = hyperbolicParametersBackstepping.copy()\n",
    "    hyperbolicParametersBacksteppingFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    hyperbolicParametersBacksteppingFixed[\"reward_class\"] = reward_class_no_penalty\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-TransportPDE1D\", **hyperbolicParametersBacksteppingFixed)\n",
    "    reward_afterQP, uBcks_afterQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_safe_list)\n",
    "\n",
    "    uBcks_afterQP_list.append(uBcks_afterQP)\n",
    "    RL_reward_afterQP.append(reward_afterQP)\n",
    "\n",
    "result = np.array([uBcks_beforeQP_list, uBcks_afterQP_list]) #(2,100,51, 100) # first 100 is num of samples, second 100 is num of 100 spatial steps\n",
    "\n",
    "condition = result[:, :,:, 0] < 1\n",
    "earliest_index = find_earliest_true(condition)\n",
    "valid_earliest_index_beforeQP = earliest_index[0,earliest_index[0,:]>=0]\n",
    "valid_earliest_index_afterQP = earliest_index[1,earliest_index[1,:]>=0]\n",
    "\n",
    "print(f\"beforeQP PF steps among {valid_earliest_index_beforeQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_beforeQP), np.std(result.shape[2] - valid_earliest_index_beforeQP))\n",
    "print(f\"afterQP PF steps among {valid_earliest_index_afterQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_afterQP), np.std(result.shape[2] - valid_earliest_index_afterQP))\n",
    "\n",
    "\n",
    "reward_result = np.array([RL_reward_beforeQP,RL_reward_afterQP])\n",
    "print(\"reward: beforeQP and afterQP\")\n",
    "print(np.mean(reward_result, axis=1))\n",
    "print(np.std(reward_result, axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdecontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
