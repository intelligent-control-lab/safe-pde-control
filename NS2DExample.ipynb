{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Make sure `PDEControlGym` is correctly installed according to [doc](https://pdecontrolgym.readthedocs.io/en/latest/guide/install.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from pde_control_gym.src import NSReward\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import stable_baselines3\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "from pde_control_gym.src import NSReward\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "from pde_control_gym.src.environments2d.navier_stokes2D import central_difference, laplace\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial condition function to be zero\n",
    "def getInitialCondition(X):\n",
    "    u = np.zeros_like(X) \n",
    "    v = np.zeros_like(X) \n",
    "    p = np.zeros_like(X) \n",
    "    return u, v, p\n",
    "\n",
    "# Set up boundary conditions here\n",
    "boundary_condition = {\n",
    "    \"upper\": [\"Controllable\", \"Dirchilet\"], \n",
    "    \"lower\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "    \"left\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "    \"right\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "}\n",
    "\n",
    "# Timestep and spatial step for PDE Solver\n",
    "T = 0.201 # To perform 200 steps\n",
    "dt = 1e-3\n",
    "dx, dy = 0.05, 0.05\n",
    "X, Y = 1, 1\n",
    "u_target = np.load('target.npz')['u']\n",
    "v_target = np.load('target.npz')['v']\n",
    "desire_states = np.stack([u_target, v_target], axis=-1) # (NT, Nx, Ny, 2)\n",
    "NS2DParameters = {\n",
    "        \"T\": T, \n",
    "        \"dt\": dt, \n",
    "        \"X\": X,\n",
    "        \"dx\": dx, \n",
    "        \"Y\": Y,\n",
    "        \"dy\":dy,\n",
    "        \"action_dim\": 1, \n",
    "        \"reward_class\": NSReward(0.1),\n",
    "        \"normalize\": False, \n",
    "        \"reset_init_condition_func\": getInitialCondition,\n",
    "        \"boundary_condition\": boundary_condition,\n",
    "        \"U_ref\": desire_states, \n",
    "        \"action_ref\": 2.0 * np.ones(1000), \n",
    "}\n",
    "\n",
    "# Make the NavierStokes PDE gym\n",
    "env = gym.make(\"PDEControlGym-NavierStokes2D\", **NS2DParameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO\n",
    "ppoModelPath = \"PPO_MODEL_PATH\"\n",
    "\n",
    "model = PPO.load(ppoModelPath)\n",
    "N_experiments = 50\n",
    "T = 200\n",
    "total_reward = 0\n",
    "for i_id in tqdm(range(N_experiments)):\n",
    "    obs, _ = env.reset(seed=i_id)\n",
    "    for t in range(T):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, _ , _  = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "print(f\"Total reward for PPO: {np.round(total_reward/N_experiments, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacModelPath = \"SAC_MODEL_PATH\"\n",
    "model = SAC.load(sacModelPath)\n",
    "N_experiments = 50\n",
    "T = 200\n",
    "total_reward = 0\n",
    "for i_id in tqdm(range(N_experiments)):\n",
    "    obs, _ = env.reset(seed=i_id)\n",
    "    for t in range(T):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, _ , _  = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "print(f\"Total reward for SAC: {np.round(total_reward/N_experiments,3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pde_control_gym.src.environments2d.navier_stokes2D import central_difference, laplace\n",
    "# Model-Based Optimization to optimize action \n",
    "def apply_boundary(a1, a2):\n",
    "    a1[:,[-1, 0]] = 0.\n",
    "    a1[[-1,0],:] = 0.\n",
    "    a2[:,[-1, 0]] = 0.\n",
    "    a2[[-1,0],:] = 0.\n",
    "    return a1, a2\n",
    "\n",
    "N_experiments = 50\n",
    "rewards = []\n",
    "for i_id in range(N_experiments):\n",
    "    np.random.seed(i_id)\n",
    "    total_reward = 0.\n",
    "    U, V = [], []\n",
    "    env.reset(seed=0)\n",
    "    for t in range(T):\n",
    "        obs, reward, done, _ , _ = env.step(np.random.uniform(2,4)) \n",
    "        U.append(env.u)\n",
    "        V.append(env.v)\n",
    "        total_reward += reward\n",
    "    u_target = np.load('target.npz')['u'][1:,:,:]\n",
    "    v_target = np.load('target.npz')['v'][1:,:,:]\n",
    "    u_ref = [2 for _ in range(T)]\n",
    "    for ite in range(1):\n",
    "        Lam1, Lam2 = [], []\n",
    "        Lam1.append(np.zeros_like(U[0]))\n",
    "        Lam2.append(np.zeros_like(U[0]))\n",
    "        pressure = np.zeros_like(U[0])\n",
    "        for t in range(T-1):\n",
    "            lam1, lam2 = Lam1[-1], Lam2[-1]\n",
    "            dl1dx, dl1dy = central_difference(lam1,\"x\",dx), central_difference(lam1, \"y\", dy)\n",
    "            dl2dx, dl2dy = central_difference(lam2,\"x\", dx), central_difference(lam2, \"y\", dy) \n",
    "            laplace_l1, laplace_l2 = laplace(lam1, dx, dy), laplace(lam2, dx, dy)\n",
    "            dlam1dt = - 2 * dl1dx * U[-1-t] - dl1dy * V[-1-t] - dl2dx * V[-1-t] - 0.1 * laplace_l1 + (U[-1-t]-u_target[-1-t])\n",
    "            dlam2dt = - 2 * dl2dy * V[-1-t] - dl1dy * U[-1-t] - dl2dx * U[-1-t] - 0.1 * laplace_l2 + (V[-1-t]-v_target[-1-t])\n",
    "            lam1 = lam1 - dt * dlam1dt\n",
    "            lam2 = lam2 - dt * dlam2dt\n",
    "            lam1, lam2 = apply_boundary(lam1, lam2)\n",
    "            pressure = env.solve_pressure(lam1, lam2, pressure)\n",
    "            lam1 = lam1 - dt * central_difference(pressure, \"x\", dx)\n",
    "            lam2 = lam2 - dt * central_difference(pressure, \"y\", dy)\n",
    "            lam1, lam2 = apply_boundary(lam1, lam2)\n",
    "            Lam1.append(lam1)\n",
    "            Lam2.append(lam2)\n",
    "        Lam1 = Lam1[::-1]\n",
    "        actions = []\n",
    "        for t in range(T):\n",
    "            dl1dx2 = central_difference(Lam1[t], \"y\", dy)\n",
    "            actions.append(u_ref[t] - 0.1/0.1 * sum(dl1dx2[-2, 12:17])*5*dx)\n",
    "        U, V = [], []\n",
    "        env.reset(seed=0)\n",
    "        total_reward = 0.\n",
    "        for t in tqdm(range(T)):\n",
    "            obs, reward, done, _ , _ = env.step(actions[t])\n",
    "            U.append(env.u)\n",
    "            V.append(env.v)\n",
    "            total_reward += reward\n",
    "        print(total_reward)\n",
    "    rewards.append(total_reward)\n",
    "print(f\"Total reward for Optimization {np.round(np.mean(rewards), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSingleEpisodeRL(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    action_list = []\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter)\n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "    u = np.array(uStorage)\n",
    "    return rew, u\n",
    "\n",
    "def RLController(obs, model):\n",
    "    action, _state = model.predict(obs)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relace `PPO_MODEL_PATH` and `SAC_MODEL_PATH` with the pretrained PPO and SAC nominal controller from `PDEControlGym`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RL models. \n",
    "ppoModelPath = \"PPO_MODEL_PATH\"\n",
    "sacModelPath = \"SAC_MODEL_PATH\"\n",
    "\n",
    "ppoModel = PPO.load(ppoModelPath)\n",
    "sacModel = SAC.load(sacModelPath)\n",
    "\n",
    "def getInitialCondition(X):\n",
    "    u = np.random.uniform(-0.1, 0.1) * np.ones_like(X) \n",
    "    v = 0 * np.ones_like(X) \n",
    "    p = 0 * np.ones_like(X) \n",
    "    return u, v, p\n",
    "\n",
    "# Set up boundary conditions here\n",
    "boundary_condition = {\n",
    "    \"upper\": [\"Controllable\", \"Dirchilet\"], \n",
    "    \"lower\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "    \"left\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "    \"right\": [\"Dirchilet\", \"Dirchilet\"], \n",
    "}\n",
    "\n",
    "# Timestep and spatial step for PDE Solver\n",
    "T = 0.201 # To perform 200 steps\n",
    "dt = 1e-3\n",
    "dx, dy = 0.05, 0.05\n",
    "X, Y = 1, 1\n",
    "u_target = np.load('target.npz')['u']\n",
    "v_target = np.load('target.npz')['v']\n",
    "desire_states = np.stack([u_target, v_target], axis=-1) # (NT, Nx, Ny, 2)\n",
    "NS2DParameters = {\n",
    "        \"T\": T, \n",
    "        \"dt\": dt, \n",
    "        \"X\": X,\n",
    "        \"dx\": dx, \n",
    "        \"Y\": Y,\n",
    "        \"dy\":dy,\n",
    "        \"action_dim\": 1, \n",
    "        \"reward_class\": NSReward(0.1),\n",
    "        \"normalize\": False, \n",
    "        \"reset_init_condition_func\": getInitialCondition,\n",
    "        \"boundary_condition\": boundary_condition,\n",
    "        \"U_ref\": desire_states, \n",
    "        \"action_ref\": 2.0 * np.ones(1000), \n",
    "}\n",
    "\n",
    "# Make the NavierStokes PDE gym\n",
    "env = gym.make(\"PDEControlGym-NavierStokes2D\", **NS2DParameters)\n",
    "\n",
    "# Model-Based Optimization to optimize action \n",
    "def apply_boundary(a1, a2):\n",
    "    a1[:,[-1, 0]] = 0.\n",
    "    a1[[-1,0],:] = 0.\n",
    "    a2[:,[-1, 0]] = 0.\n",
    "    a2[[-1,0],:] = 0.\n",
    "    return a1, a2\n",
    "\n",
    "total_reward = 0.\n",
    "U, V = [], []\n",
    "T = 200\n",
    "\n",
    "rewards = []\n",
    "times = []\n",
    "for experiment_i in range(1):\n",
    "    # np.random.seed(experiment_i)\n",
    "    # env.reset(seed=400)\n",
    "    env.reset()\n",
    "    s = time.time()\n",
    "    for t in tqdm(range(T)):\n",
    "        obs, reward, done, _ , _ = env.step(np.random.uniform(2,4)) \n",
    "        U.append(env.u)\n",
    "        V.append(env.v)\n",
    "        total_reward += reward\n",
    "    print(\"Total Reward random:\", total_reward)\n",
    "u_target = np.load('target.npz')['u']\n",
    "v_target = np.load('target.npz')['v']\n",
    "u_ref = [2 for _ in range(T)]\n",
    "xs_opt = []\n",
    "ys_opt = []\n",
    "xs_ppo = []\n",
    "ys_ppo = []\n",
    "xs_sac = []\n",
    "ys_sac = []\n",
    "for ite in tqdm(range(5000)):\n",
    "    env.reset()\n",
    "    Lam1, Lam2 = [], []\n",
    "    Lam1.append(np.zeros_like(U[0]))\n",
    "    Lam2.append(np.zeros_like(U[0]))\n",
    "    pressure = np.zeros_like(U[0])\n",
    "    for t in (range(T-1)):\n",
    "        lam1, lam2 = Lam1[-1], Lam2[-1]\n",
    "        dl1dx, dl1dy = central_difference(lam1,\"x\",dx), central_difference(lam1, \"y\", dy)\n",
    "        dl2dx, dl2dy = central_difference(lam2,\"x\", dx), central_difference(lam2, \"y\", dy) \n",
    "        laplace_l1, laplace_l2 = laplace(lam1, dx, dy), laplace(lam2, dx, dy)\n",
    "        dlam1dt = - 2 * dl1dx * U[-1-t] - dl1dy * V[-1-t] - dl2dx * V[-1-t] - 0.1 * laplace_l1 + (U[-1-t]-u_target[-1-t])\n",
    "        dlam2dt = - 2 * dl2dy * V[-1-t] - dl1dy * U[-1-t] - dl2dx * U[-1-t] - 0.1 * laplace_l2 + (V[-1-t]-v_target[-1-t])\n",
    "        lam1 = lam1 - dt * dlam1dt\n",
    "        lam2 = lam2 - dt * dlam2dt\n",
    "        lam1, lam2 = apply_boundary(lam1, lam2)\n",
    "        pressure = env.solve_pressure(lam1, lam2, pressure)\n",
    "        lam1 = lam1 - dt * central_difference(pressure, \"x\", dx)\n",
    "        lam2 = lam2 - dt * central_difference(pressure, \"y\", dy)\n",
    "        lam1, lam2 = apply_boundary(lam1, lam2)\n",
    "        Lam1.append(lam1)\n",
    "        Lam2.append(lam2)\n",
    "    Lam1 = Lam1[::-1]\n",
    "    actions = []\n",
    "    for t in (range(T)):\n",
    "        dl1dx2 = central_difference(Lam1[t], \"y\", dy)\n",
    "        actions.append(u_ref[t] - 0.1/0.1 * sum(dl1dx2[-2, :])*5*dx)\n",
    "    U, V = [], []\n",
    "    # env.reset(seed=400)\n",
    "    env.reset()\n",
    "    total_reward = 0.\n",
    "    for t in (range(T)):\n",
    "        obs, reward, done, _ , _ = env.step(actions[t]) # actions === env.U[1:,-1,1,0]\n",
    "        U.append(env.u)\n",
    "        V.append(env.v)\n",
    "        total_reward += reward\n",
    "        print(env.U[100,-2,10,0])\n",
    "    xs_opt.append(env.U[:,-1,1,0])\n",
    "    ys_opt.append(env.U[:,-2,10,0])\n",
    "    _ppo_r, _u_ppo = runSingleEpisodeRL(RLController, env, ppoModel)\n",
    "    xs_ppo.append(_u_ppo[:,-1,1,0])\n",
    "    ys_ppo.append(_u_ppo[:,-2,10,0])\n",
    "    \n",
    "    _sac_r, _u_sac = runSingleEpisodeRL(RLController, env, sacModel)\n",
    "    xs_sac.append(_u_sac[:,-1,1,0])\n",
    "    ys_sac.append(_u_sac[:,-2,10,0])\n",
    "\n",
    "    \n",
    "\n",
    "print(np.stack(xs_opt).shape,np.stack(ys_opt).shape)\n",
    "\n",
    "print(np.stack(xs_ppo).shape,np.stack(ys_ppo).shape)\n",
    "\n",
    "\n",
    "print(np.stack(xs_sac).shape,np.stack(ys_sac).shape)\n",
    "data_opt = {\"a\": np.stack(xs_opt), \"u\": np.stack(ys_opt)}\n",
    "scipy.io.savemat(\"data_opt_ns.mat\", data_opt)\n",
    "\n",
    "data_ppo = {\"a\": np.stack(xs_ppo), \"u\": np.stack(ys_ppo)}\n",
    "scipy.io.savemat(\"data_ppo_ns.mat\", data_ppo)\n",
    "\n",
    "data_sac = {\"a\": np.stack(xs_sac), \"u\": np.stack(ys_sac)}\n",
    "scipy.io.savemat(\"data_sac_ns.mat\", data_sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def runSingleEpisodeQP(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter,i)\n",
    "        \n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "        i += 1\n",
    "    u = np.array(uStorage)\n",
    "    return rew, u\n",
    "def QP_filter_Controller(obs, parameter,index):\n",
    "    return parameter[index+1]\n",
    "\n",
    "def find_earliest_true(condition):\n",
    "    # Iterate over the first two dimensions (10 and 8) and check for each slice\n",
    "    earliest_indices = np.full(condition.shape[:2], 0)  # Initialize with -1 (indicating no valid index)\n",
    "\n",
    "    for i in range(condition.shape[0]):  # Iterate over first dimension\n",
    "        for j in range(condition.shape[1]):  # Iterate over second dimension\n",
    "            # For each slice (i, j), find the earliest index where the condition is True\n",
    "            # and all subsequent values are also True\n",
    "            for k in range(condition.shape[2]):\n",
    "                if not condition[i, j, condition.shape[2]-k-1]: \n",
    "                    if k == 0:\n",
    "                        earliest_indices[i,j] = -1\n",
    "                    else:\n",
    "                        earliest_indices[i,j] = condition.shape[2]-k\n",
    "                    break\n",
    "    return earliest_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the filtered results `FILTER_RESULT_PATH` with the one saved in  `test_cbf_ns.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RL_1000 = np.load(\"FILTER_RESULT_PATH\")\n",
    "RL_reward_beforeQP = []\n",
    "RL_reward_afterQP = []\n",
    "uBcks_beforeQP_list = []\n",
    "uBcks_afterQP_list = []\n",
    "u_target = np.load('target.npz')['u']\n",
    "v_target = np.load('target.npz')['v']\n",
    "\n",
    "for i in range(RL_1000[\"safe_label\"].transpose().shape[0]):\n",
    "\n",
    "    U_list = RL_1000[\"U_nominal\"][:, i]\n",
    "    Y_list = RL_1000[\"Y_nominal\"][:, i]\n",
    "    print(i)\n",
    "\n",
    "    def getInitialConditionFixed(X):\n",
    "        u = U_list[0] * np.ones_like(X) \n",
    "        v = 0 * np.ones_like(X) \n",
    "        p = 0 * np.ones_like(X) \n",
    "        return u, v, p\n",
    "    NS2DParametersFixed = NS2DParameters.copy()\n",
    "    NS2DParametersFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-NavierStokes2D\", **NS2DParametersFixed)\n",
    "    reward_beforeQP, uBcks_beforeQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_list)\n",
    "    \n",
    "    uBcks_beforeQP_list.append(uBcks_beforeQP[:,-2,10,0])\n",
    "    RL_reward_beforeQP.append(reward_beforeQP)\n",
    "\n",
    "    U_safe_list = RL_1000[\"U_safe\"][:, i]\n",
    "    def getInitialConditionFixed(X):\n",
    "        u = U_list[0] * np.ones_like(X) \n",
    "        v = 0 * np.ones_like(X) \n",
    "        p = 0 * np.ones_like(X) \n",
    "        return u, v, p\n",
    "    NS2DParametersFixed = NS2DParameters.copy()\n",
    "    NS2DParametersFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-NavierStokes2D\", **NS2DParametersFixed)\n",
    "    reward_afterQP, uBcks_afterQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_safe_list)\n",
    "\n",
    "    uBcks_afterQP_list.append(uBcks_afterQP[:,-2,10,0])\n",
    "\n",
    "    RL_reward_afterQP.append(reward_afterQP)\n",
    "    print(reward_beforeQP,reward_afterQP)\n",
    "\n",
    "\n",
    "result = np.array([uBcks_beforeQP_list, uBcks_afterQP_list]) \n",
    "print(result.shape)\n",
    "\n",
    "condition = (result[:, :,:]-u_target[:,-2,10] < 0.145) & (result[:, :,:]-u_target[:,-2,10] > -0.145)\n",
    "earliest_index = find_earliest_true(condition)\n",
    "valid_earliest_index_beforeQP = earliest_index[0,earliest_index[0,:]>=0]\n",
    "valid_earliest_index_afterQP = earliest_index[1,earliest_index[1,:]>=0]\n",
    "\n",
    "print(f\"beforeQP PF steps among {valid_earliest_index_beforeQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_beforeQP), np.std(result.shape[2] - valid_earliest_index_beforeQP))\n",
    "print(f\"afterQP PF steps among {valid_earliest_index_afterQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_afterQP), np.std(result.shape[2] - valid_earliest_index_afterQP))\n",
    "\n",
    "reward_result = np.array([RL_reward_beforeQP,RL_reward_afterQP])\n",
    "print(\"reward: beforeQP and afterQP\")\n",
    "print(np.mean(reward_result, axis=1))\n",
    "print(np.std(reward_result, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdecontrol",
   "language": "python",
   "name": "pdecontrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
