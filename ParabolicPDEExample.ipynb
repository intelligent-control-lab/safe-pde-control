{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9b8501",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Make sure `PDEControlGym` is correctly installed according to [doc](https://pdecontrolgym.readthedocs.io/en/latest/guide/install.html). This file is modified based on `PDEControlGym/examples/reactionDiffusionPDE/ParabolicPDEExample.ipynb` from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8ab6d-b602-496c-968e-948d3d5dbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import stable_baselines3\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import pde_control_gym\n",
    "from pde_control_gym.src import TunedReward1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307a8313-126e-479f-b94c-69dbf2188e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version 0.29.1\n",
      "Numpy version 1.26.2\n",
      "Stable Baselines3 version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Print Versioning\n",
    "print(\"Gym version\", gym.__version__)\n",
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"Stable Baselines3 version\", stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9efd4f-b0e3-4c70-9c15-6e677066c310",
   "metadata": {},
   "source": [
    "Helper Functions for the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32177259-8979-4957-baf5-e8f15c9c563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NOISE\n",
    "def noiseFunc(state):\n",
    "    return state\n",
    "\n",
    "# Chebyshev Polynomial Beta Functions\n",
    "def solveBetaFunction(x, gamma):\n",
    "    beta = np.zeros(len(x), dtype=np.float32)\n",
    "    for idx, val in enumerate(x):\n",
    "        beta[idx] = 50*math.cos(gamma*math.acos(val))\n",
    "    return beta\n",
    "\n",
    "# Kernel function solver for backstepping\n",
    "def solveKernelFunction(beta):\n",
    "    k = np.zeros((len(beta), len(beta)))\n",
    "    # First we calculate a at each timestep\n",
    "    a = beta\n",
    "\n",
    "    # FD LOOP\n",
    "    k[1][1] = -(a[1] + a[0]) * dx / 4\n",
    "    for i in range(1, len(beta)-1):\n",
    "        k[i+1][0] = 0\n",
    "        k[i+1][i+1] = k[i][i]-dx/4.0*(a[i-1] + a[i])\n",
    "        k[i+1][i] = k[i][i] - dx/2 * a[i]\n",
    "        for j in range(1, i):\n",
    "                k[i+1][j] = -k[i-1][j] + k[i][j+1] + k[i][j-1] + a[j]*(dx**2)*(k[i][j+1]+k[i][j-1])/2\n",
    "    return k\n",
    "\n",
    "# Control convolution solver\n",
    "def solveControl(kernel, u):\n",
    "    return sum(kernel[-1][0:len(u)-1]*u[0:len(u)-1])*dx\n",
    "\n",
    "# Set initial condition function here\n",
    "def getInitialCondition(nx):\n",
    "    return np.ones(nx+1)*np.random.uniform(1, 10)\n",
    "\n",
    "# Returns beta functions passed into PDE environment. Currently gamma is always\n",
    "# set to 8, but this can be modified for further problems\n",
    "def getBetaFunction(nx):\n",
    "    return solveBetaFunction(np.linspace(0, 1, nx+1), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1f129a-e88d-48d1-96ba-da0c85abda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep and spatial step for PDE Solver\n",
    "T = 1\n",
    "dt = 1e-5\n",
    "dx = 5e-3\n",
    "X = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac9fc8b-2b8f-4e1b-a46b-64222b995b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backstepping does not need to normalize actions to be between -1 and 1, so normalize is set to False. Otherwise, \n",
    "# parameters are same as RL algorithms\n",
    "parabolicParameters = {\n",
    "        \"T\": T, \n",
    "        \"dt\": dt, \n",
    "        \"X\": X,\n",
    "        \"dx\": dx, \n",
    "        \"reward_class\": TunedReward1D(int(round(T/dt)), -1e3, 3e2),\n",
    "        \"normalize\": None,\n",
    "        \"sensing_loc\": \"full\", \n",
    "        \"control_type\": \"Dirchilet\", \n",
    "        \"sensing_type\": None,\n",
    "        \"sensing_noise_func\": lambda state: state,\n",
    "        \"limit_pde_state_size\": True,\n",
    "        \"max_state_value\": 1e10,\n",
    "        \"max_control_value\": 20,\n",
    "        \"reset_init_condition_func\": getInitialCondition,\n",
    "        \"reset_recirculation_func\": getBetaFunction,\n",
    "        \"control_sample_rate\": 0.001,\n",
    "}\n",
    "\n",
    "parabolicParametersBackstepping = parabolicParameters.copy()\n",
    "parabolicParametersBackstepping[\"normalize\"] = False\n",
    "\n",
    "parabolicParametersRL = parabolicParameters.copy()\n",
    "parabolicParametersRL[\"normalize\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acc50f",
   "metadata": {},
   "source": [
    "Relace `PPO_MODEL_PATH` and `SAC_MODEL_PATH` with the pretrained PPO and SAC nominal controller from `PDEControlGym`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bd8f0-7f37-404d-bb6c-f857f0fd3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RL models. \n",
    "ppoModelPath = \"PPO_MODEL_PATH\"\n",
    "sacModelPath = \"SAC_MODEL_PATH\"\n",
    "\n",
    "ppoModel = PPO.load(ppoModelPath)\n",
    "sacModel = SAC.load(sacModelPath)\n",
    "\n",
    "# For backstepping controller\n",
    "spatial = np.linspace(dx, X, int(round(X/dx)))\n",
    "beta = solveBetaFunction(spatial, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93c80f-723e-4fc0-98a5-deeb0db023ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a single epsiode calculation\n",
    "# Parameter varies. For SAC and PPO it is the model itself\n",
    "# For backstepping it is the beta function\n",
    "def runSingleEpisode(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter)\n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "    u = np.array(uStorage)\n",
    "    return rew, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49278e04-7771-4a5c-9d75-0ca82a512b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Controllers\n",
    "def bcksController(obs, kernel):\n",
    "    return solveControl(kernel, obs)\n",
    "\n",
    "def RLController(obs, model):\n",
    "    action, _state = model.predict(obs)\n",
    "    return action\n",
    "\n",
    "def openLoopController(_, _a):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfbc03",
   "metadata": {},
   "source": [
    "Collect data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4398e-f756-411c-9618-f445940d8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# from tqdm import tqdm\n",
    "from tqdm import trange, tqdm\n",
    "def getInitialConditionRandom(nx):\n",
    "    return np.ones(nx+1) * (1 + np.random.rand() * 9)\n",
    "\n",
    "\n",
    "\n",
    "parabolicParametersBacksteppingRandom = parabolicParametersBackstepping.copy()\n",
    "parabolicParametersBacksteppingRandom[\"reset_init_condition_func\"] = getInitialConditionRandom\n",
    "\n",
    "\n",
    "\n",
    "parabolicParametersRLRandom = parabolicParametersRL.copy()\n",
    "parabolicParametersRLRandom[\"reset_init_condition_func\"] = getInitialConditionRandom\n",
    "\n",
    "\n",
    "# Make environments\n",
    "envBcksRandom = gym.make(\"PDEControlGym-ReactionDiffusionPDE1D\", **parabolicParametersBacksteppingRandom)\n",
    "\n",
    "\n",
    "envRLRandom = gym.make(\"PDEControlGym-ReactionDiffusionPDE1D\", **parabolicParametersRLRandom)\n",
    "\n",
    "xs_bcks = []\n",
    "ys_bcks = []\n",
    "xs_ppo = []\n",
    "ys_ppo = []\n",
    "xs_sac = []\n",
    "ys_sac = []\n",
    "xs_bcks_dense = []\n",
    "ys_bcks_dense = []\n",
    "xs_ppo_dense = []\n",
    "ys_ppo_dense = []\n",
    "xs_sac_dense = []\n",
    "ys_sac_dense = []\n",
    "for i in range(15000):\n",
    "    \n",
    "    rewBcksRandom, uBcksRandom = runSingleEpisode(bcksController, envBcksRandom, kernel)\n",
    "    xs_bcks.append((uBcksRandom.transpose()[:,::20])[-1])\n",
    "    ys_bcks.append((uBcksRandom.transpose()[:,::20])[100])\n",
    "\n",
    "    rewPPORandom, uPPORandom = runSingleEpisode(RLController, envRLRandom, ppoModel)\n",
    "    xs_ppo.append((uPPORandom.transpose()[:,::20])[-1])\n",
    "    ys_ppo.append((uPPORandom.transpose()[:,::20])[100])\n",
    "    \n",
    "\n",
    "    rewSACRandom, uSACRandom = runSingleEpisode(RLController, envRLRandom, sacModel)\n",
    "    xs_sac.append((uSACRandom.transpose()[:,::20])[-1])\n",
    "    ys_sac.append((uSACRandom.transpose()[:,::20])[100])\n",
    "    if i % 1000 == 0: print(i)\n",
    "    # xs_bcks_dense.append((uBcksRandom.transpose())[-1])\n",
    "    # ys_bcks_dense.append((uBcksRandom.transpose())[100])\n",
    "\n",
    "    # xs_ppo_dense.append((uPPORandom.transpose())[-1])\n",
    "    # ys_ppo_dense.append((uPPORandom.transpose())[100])\n",
    "\n",
    "    # xs_sac_dense.append((uSACRandom.transpose())[-1])\n",
    "    # ys_sac_dense.append((uSACRandom.transpose())[100])\n",
    "\n",
    "        \n",
    "data_bcks = {\"a\": np.stack(xs_bcks), \"u\": np.stack(ys_bcks)}\n",
    "scipy.io.savemat(\"data_bcks_parabolic_train3.mat\", data_bcks)\n",
    "\n",
    "data_ppo = {\"a\": np.stack(xs_ppo), \"u\": np.stack(ys_ppo)}\n",
    "scipy.io.savemat(\"data_ppo_parabolic_train3.mat\", data_ppo)\n",
    "\n",
    "data_sac = {\"a\": np.stack(xs_sac), \"u\": np.stack(ys_sac)}\n",
    "scipy.io.savemat(\"data_sac_parabolic_train3.mat\", data_sac)\n",
    "\n",
    "# data_bcks_dense = {\"a\": np.stack(xs_bcks_dense), \"u\": np.stack(ys_bcks_dense)}\n",
    "# scipy.io.savemat(\"data_bcks_parabolic_test_dense.mat\", data_bcks_dense)\n",
    "\n",
    "# data_ppo_dense = {\"a\": np.stack(xs_ppo_dense), \"u\": np.stack(ys_ppo_dense)}\n",
    "# scipy.io.savemat(\"data_ppo_parabolic_test_dense.mat\", data_ppo_dense)\n",
    "\n",
    "# data_sac_dense = {\"a\": np.stack(xs_sac_dense), \"u\": np.stack(ys_sac_dense)}\n",
    "# scipy.io.savemat(\"data_sac_parabolic_test_dense.mat\", data_sac_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc07333",
   "metadata": {},
   "source": [
    "# Safety filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aeeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSingleEpisodeQP(model, env, parameter):\n",
    "    terminate = False\n",
    "    truncate = False\n",
    "\n",
    "    # Holds the resulting states\n",
    "    uStorage = []\n",
    "\n",
    "    # Reset Environment\n",
    "    obs,__ = env.reset()\n",
    "    uStorage.append(obs)\n",
    "\n",
    "    i = 0\n",
    "    rew = 0\n",
    "    while not truncate and not terminate:\n",
    "        # use backstepping controller\n",
    "        action = model(obs, parameter,i)\n",
    "\n",
    "        obs, rewards, terminate, truncate, info = env.step(action)\n",
    "\n",
    "        uStorage.append(obs)\n",
    "        rew += rewards \n",
    "        i += 1\n",
    "    u = np.array(uStorage)\n",
    "    return rew, u\n",
    "\n",
    "def QP_filter_Controller(obs, parameter,index):\n",
    "    return parameter[index+1]\n",
    "\n",
    "def find_earliest_true(condition):\n",
    "    # Iterate over the first two dimensions (10 and 8) and check for each slice\n",
    "    earliest_indices = np.full(condition.shape[:2], 0)  # Initialize with -1 (indicating no valid index)\n",
    "\n",
    "    for i in range(condition.shape[0]):  # Iterate over first dimension\n",
    "        for j in range(condition.shape[1]):  # Iterate over second dimension\n",
    "            # For each slice (i, j), find the earliest index where the condition is True\n",
    "            # and all subsequent values are also True\n",
    "            for k in range(condition.shape[2]):\n",
    "                if not condition[i, j, condition.shape[2]-k-1]: \n",
    "                    if k == 0:\n",
    "                        earliest_indices[i,j] = -1\n",
    "                    else:\n",
    "                        earliest_indices[i,j] = condition.shape[2]-k\n",
    "                    break\n",
    "    return earliest_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae364d88",
   "metadata": {},
   "source": [
    "Replace the filtered results `FILTER_RESULT_PATH` with the one saved in  `test_cbf_parabolic.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3dd5a-c0dc-496a-9a6e-d7f1dba7e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_1000 = np.load(\"FILTER_RESULT_PATH\")\n",
    "RL_reward_beforeQP = []\n",
    "RL_reward_afterQP = []\n",
    "uBcks_beforeQP_list = []\n",
    "uBcks_afterQP_list = []\n",
    "uBcks_beforeQP,uBcks_afterQP = 0,0\n",
    "for i in range(RL_1000[\"safe_label\"].transpose().shape[0]):\n",
    "    U_list = RL_1000[\"U_nominal\"][:, i]\n",
    "    def getInitialConditionFixed(nx):\n",
    "        return np.ones(nx+1) * U_list[0]\n",
    "    parabolicParametersBacksteppingFixed = parabolicParametersBackstepping.copy()\n",
    "    parabolicParametersBacksteppingFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-ReactionDiffusionPDE1D\", **parabolicParametersBacksteppingFixed)\n",
    "    \n",
    "    \n",
    "    reward_beforeQP, uBcks_beforeQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_list)\n",
    "\n",
    "\n",
    "    U_safe_list = RL_1000[\"U_safe\"][:, i]\n",
    "    def getInitialConditionFixed(nx):\n",
    "        return np.ones(nx+1) * U_list[0]\n",
    "    parabolicParametersBacksteppingFixed = parabolicParametersBackstepping.copy()\n",
    "    parabolicParametersBacksteppingFixed[\"reset_init_condition_func\"] = getInitialConditionFixed\n",
    "    envBcksFixed = gym.make(\"PDEControlGym-ReactionDiffusionPDE1D\", **parabolicParametersBacksteppingFixed)\n",
    "    \n",
    "    \n",
    "    reward_afterQP, uBcks_afterQP = runSingleEpisodeQP(QP_filter_Controller, envBcksFixed, U_safe_list)\n",
    "    RL_reward_beforeQP.append(reward_beforeQP)\n",
    "    uBcks_beforeQP_list.append(uBcks_beforeQP)\n",
    "\n",
    "    RL_reward_afterQP.append(reward_afterQP)\n",
    "    uBcks_afterQP_list.append(uBcks_afterQP)\n",
    "\n",
    "\n",
    "result = np.array([uBcks_beforeQP_list, uBcks_afterQP_list]) \n",
    "\n",
    "condition = result[:, :,:, 100] < 0.6\n",
    "earliest_index = find_earliest_true(condition)\n",
    "valid_earliest_index_beforeQP = earliest_index[0,earliest_index[0,:]>=0]\n",
    "valid_earliest_index_afterQP = earliest_index[1,earliest_index[1,:]>=0]\n",
    "\n",
    "print(f\"beforeQP PF steps among {valid_earliest_index_beforeQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_beforeQP), np.std(result.shape[2] - valid_earliest_index_beforeQP))\n",
    "print(f\"afterQP PF steps among {valid_earliest_index_afterQP.shape[0]} PF trajectories\", np.mean(result.shape[2] - valid_earliest_index_afterQP), np.std(result.shape[2] - valid_earliest_index_afterQP))\n",
    "\n",
    "\n",
    "reward_result = np.array([RL_reward_beforeQP,RL_reward_afterQP])\n",
    "print(\"reward: beforeQP and afterQP\")\n",
    "print(np.mean(reward_result, axis=1))\n",
    "print(np.std(reward_result, axis=1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdecontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
