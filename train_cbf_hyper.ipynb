{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf555e98-8e13-4f75-8272-4984ca0d448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Burgers\n",
    "using DataDeps, MAT, MLUtils\n",
    "using NeuralOperators, Flux\n",
    "using BSON\n",
    "using DataDeps, MAT, MLUtils\n",
    "using NeuralOperators, Flux\n",
    "using CUDA, FluxTraining, BSON\n",
    "import Flux: params\n",
    "using BSON: @save, @load\n",
    "using ProgressBars\n",
    "using Zygote\n",
    "using Optimisers, ParameterSchedulers\n",
    "\n",
    "using Burgers\n",
    "using FluxTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0bc45-89b3-432f-a24d-85cb2efc2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_get_data(file_path; n = 50000, Δsamples = 1, grid_size = div(51, Δsamples), T = Float32)\n",
    "    file = matopen(file_path)\n",
    "    \n",
    "    x_data = T.(collect(read(file, \"a\")[1:n, 1:Δsamples:end]'))\n",
    "    y_data = T.(collect(read(file, \"u\")[1:n, 1:Δsamples:end]'))\n",
    "    safe_labels = T.(collect(read(file, \"safe\")[1:n, 1:Δsamples:end]'))\n",
    "    pf_labels = T.(collect(read(file, \"pf\")[1:n, 1:Δsamples:end]'))\n",
    "    close(file)\n",
    "\n",
    "    x_loc_data = Array{T, 3}(undef, 2, grid_size, n)\n",
    "    x_loc_data[1, :, :] .= reshape(repeat(LinRange(0, 5, grid_size), n), (grid_size, n))\n",
    "    x_loc_data[2, :, :] .= x_data\n",
    "\n",
    "    return x_loc_data, reshape(y_data, 1, :, n), safe_labels, pf_labels\n",
    "end\n",
    "\n",
    "function my_get_dataloader(; ratio::Float64 = 0.9, batchsize = 128)\n",
    "    𝐱1, 𝐲1, safe1, pf1 = my_get_data(\"data_bcks_hyperbolic_1_minus.mat\") \n",
    "    \n",
    "    data_train1, data_test1 = splitobs((𝐱1, 𝐲1, safe1, pf1), at = ratio)\n",
    "    𝐱2, 𝐲2, safe2, pf2 = my_get_data(\"data_bcks_hyperbolic_1_minus.mat\")\n",
    "    \n",
    "    data_train2, data_test2 = splitobs((𝐱2, 𝐲2, safe2, pf2), at = ratio)\n",
    "    𝐱3, 𝐲3, safe3, pf3 = my_get_data(\"data_bcks_hyperbolic_1_minus.mat\")\n",
    "    \n",
    "    data_train3, data_test3 = splitobs((𝐱3, 𝐲3, safe3, pf3), at = ratio)\n",
    "\n",
    "    @show size(data_train3[1]), size(data_test3[2])\n",
    "\n",
    "    data_train1_x_pf = data_train1[1][:,:,:]\n",
    "    data_test1_x_pf = data_test1[1][:,:,:]\n",
    "    data_train1_y_pf = data_train1[2][:,:,:]\n",
    "    data_test1_y_pf = data_test1[2][:,:,:]\n",
    "    data_train1_safe_pf = data_train1[3][:,:]\n",
    "    data_test1_safe_pf = data_test1[3][:,:]\n",
    "\n",
    "    data_train2_x_pf = data_train2[1][:,:,:]\n",
    "    data_test2_x_pf = data_test2[1][:,:,:]\n",
    "    data_train2_y_pf = data_train2[2][:,:,:]\n",
    "    data_test2_y_pf = data_test2[2][:,:,:]\n",
    "    data_train2_safe_pf = data_train2[3][:,:]\n",
    "    data_test2_safe_pf = data_test2[3][:,:]\n",
    "\n",
    "    data_train3_x_pf = data_train3[1][:,:,:]\n",
    "    data_test3_x_pf = data_test3[1][:,:,:]\n",
    "    data_train3_y_pf = data_train3[2][:,:,:]\n",
    "    data_test3_y_pf = data_test3[2][:,:,:]\n",
    "    data_train3_safe_pf = data_train3[3][:,:]\n",
    "    data_test3_safe_pf = data_test3[3][:,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_train = (cat(cat(data_train1_x_pf, data_train2_x_pf, dims=3), data_train3_x_pf, dims=3), \n",
    "                    cat(cat(data_train1_y_pf, data_train2_y_pf, dims=3), data_train3_y_pf, dims=3), \n",
    "                    cat(cat(data_train1_safe_pf, data_train2_safe_pf, dims=2), data_train3_safe_pf, dims=2)) # omit the last pf tumple\n",
    "    data_test = (cat(cat(data_test1_x_pf, data_test2_x_pf, dims=3), data_test3_x_pf, dims=3), \n",
    "                cat(cat(data_test1_y_pf, data_test2_y_pf, dims=3), data_test3_y_pf, dims=3), \n",
    "                cat(cat(data_test1_safe_pf, data_test2_safe_pf, dims=2), data_test3_safe_pf, dims=2)) # # omit the last pf tumple\n",
    "    loader_train = DataLoader(data_train, batchsize = batchsize, shuffle = true)\n",
    "    loader_test = DataLoader(data_test, batchsize = batchsize, shuffle = false)\n",
    "\n",
    "    return loader_train, loader_test\n",
    "end\n",
    "function delete_with_probability!(list, p = 0.5)\n",
    "    mask = rand(length(list)) .< p  \n",
    "    index = findall(x->x==1, mask)\n",
    "    return list[index] \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5135bc-4000-4f86-bf28-f9a56a9ded88",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_naive_safeset(ϕ, x,y_init)\n",
    "    @show x[:, 1:10], ϕ(x)[1, 1:10], y_init[1:10]\n",
    "    @show x[:, end-10:end], ϕ(x)[1, end-10:end], y_init[end-10:end]\n",
    "    index = findall(x->x==0, y_init)\n",
    "    size(index)[1] == 0 && return 0\n",
    "    x = x[:, index]\n",
    "    y_init = y_init[index]\n",
    "    \n",
    "    loss = relu((2 .* y_init .- 1) .* ϕ(x)[1, :] .+ 1e-6)\n",
    "    return (sum(loss)) / (size(loss)[end])\n",
    "end\n",
    "\n",
    "function loss_regularization(ϕ::Chain, x::AbstractArray,y_init::AbstractArray)\n",
    "     # safe: 1; unsafe: 0\n",
    "    index = findall(x->x==0, y_init)\n",
    "    size(index)[1] == 0 && return 0\n",
    "    x = x[:, index]\n",
    "    y_init = y_init[index]\n",
    "    loss = sigmoid_fast((2 .* y_init .- 1) .* ϕ(x)[1, :])\n",
    "    return sum(loss) / (size(loss)[end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6234f0c-bafc-4a91-b737-61ab953af6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_naive_safeset_end(ϕ, x,y_init;minus_safe=false)\n",
    "    if minus_safe\n",
    "        index = findall(x->x==1, y_init)\n",
    "        size(index)[1] == 0 && return 0\n",
    "        x = x[:, index]\n",
    "        y_init = y_init[index]\n",
    "        \n",
    "        loss = relu((2 .* y_init .- 1) .* ϕ(x)[1, :] .+ 1e-6)\n",
    "        return (sum(loss)) / (size(loss)[end])\n",
    "    else\n",
    "        return relu((2 .* y_init[end] .- 1) .* ϕ(x)[1, end] .+ 1e-6)\n",
    "    end\n",
    "end\n",
    "\n",
    "function loss_regularization_end(ϕ::Chain, x::AbstractArray,y_init::AbstractArray;minus_safe=false)\n",
    "    if minus_safe\n",
    "        index = findall(x->x==1, y_init)\n",
    "        size(index)[1] == 0 && return 0\n",
    "        x = x[:, index]\n",
    "        y_init = y_init[index]\n",
    "        loss = sigmoid_fast((2 .* y_init .- 1) .* ϕ(x)[1, :])\n",
    "        return sum(loss) / (size(loss)[end])\n",
    "    else\n",
    "        return sigmoid_fast((2 .* y_init[end] .- 1) .* ϕ(x)[1, end])\n",
    "    end\n",
    "end\n",
    "\n",
    "function find_derivative(vector)\n",
    "    M, N = size(vector)[2], size(vector)[3]\n",
    "\n",
    "    # Assume `vector` is the (2, M, N) array\n",
    "    inputs = vector[1, :, :]  # Shape (M, N)\n",
    "    outputs = vector[2, :, :]  # Shape (M, N)\n",
    "\n",
    "    # Preallocate the derivative array with shape (1, M, N)\n",
    "    derivatives = zeros(Float64, 1, M, N)\n",
    "\n",
    "    # Central differences for the interior points (2 to M-1)\n",
    "    derivatives[1, 2:M-1, :] = (outputs[3:M, :] .- outputs[1:M-2, :]) ./ (inputs[3:M, :] .- inputs[1:M-2, :])\n",
    "\n",
    "    # Forward difference for the first point\n",
    "    derivatives[1, 1, :] = (outputs[2, :] .- outputs[1, :]) ./ (inputs[2, :] .- inputs[1, :])\n",
    "\n",
    "    # Backward difference for the last point\n",
    "    derivatives[1, M, :] = (outputs[M, :] .- outputs[M-1, :]) ./ (inputs[M, :] .- inputs[M-1, :])\n",
    "\n",
    "    # `derivatives` now contains the derivative of the output with respect to the input\n",
    "    # with shape (1, M, N)\n",
    "    return derivatives\n",
    "end\n",
    "\n",
    "function find_derivative_1step(vector)\n",
    "    M, N = size(vector)[2], size(vector)[3]\n",
    "\n",
    "    # Assume `vector` is the (2, M, N) array\n",
    "    inputs = vector[1, :, :]  # Shape (M, N)\n",
    "    outputs = vector[2, :, :]  # Shape (M, N)\n",
    "\n",
    "    # Preallocate the derivative array with shape (1, M, N)\n",
    "    derivatives = zeros(Float64, 1, M, N)\n",
    "\n",
    "    # 1-step forward finite difference for all points from 1 to M-1\n",
    "    derivatives[1, 1:M-1, :] = (outputs[2:M, :] .- outputs[1:M-1, :]) ./ (inputs[2:M, :] .- inputs[1:M-1, :])\n",
    "\n",
    "    # 1-step backward finite difference for the last point\n",
    "    derivatives[1, M, :] = (outputs[M, :] .- outputs[M-1, :]) ./ (inputs[M, :] .- inputs[M-1, :])\n",
    "\n",
    "    # `derivatives` now contains the derivative of the output with respect to the input\n",
    "    # with shape (1, M, N)\n",
    "    return derivatives\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function loss_pf(ϕ::Chain, U::AbstractArray, Yt::AbstractArray, U_0,extended_U̇, ∇Y_t,T, α,y_init; all=false,ϵ = 0.5,λ_pf_batch=nothing)\n",
    "    ∇Y_t = reshape(∇Y_t, size(Yt))\n",
    "    isnothing(λ_pf_batch) || (λ_pf_batch = reshape(λ_pf_batch, size(U_0[1:1,:])))\n",
    "    \n",
    "    if !all\n",
    "        mask = abs.(Yt[2,:]) .< ϵ\n",
    "        index = findall(x->x==true, mask)\n",
    "        index = delete_with_probability!(index, 0.2) \n",
    "        size(index)[1] == 0 && return 0\n",
    "        Yt = Yt[:, index]\n",
    "    \n",
    "        ∇Y_t = ∇Y_t[:, index]\n",
    "        U_0 = U_0[:, index]\n",
    "        isnothing(λ_pf_batch) || (λ_pf_batch = λ_pf_batch[:, index])\n",
    "    end\n",
    "\n",
    "    \n",
    "    state_dim, batchsize = size(Yt) # 2*51000\n",
    "    _, ∇ϕ = Zygote.pullback(ϕ, Yt)\n",
    "    ∇ϕ_Y = ∇ϕ(ones(size(Yt)))[1] ./ state_dim\n",
    "    ∇ϕ_Y = reshape(∇ϕ_Y, (1, state_dim, batchsize))\n",
    "\n",
    "    ∇Y_t = reshape(∇Y_t, (state_dim, 1, batchsize))\n",
    "    \n",
    "    ϕ̇ = reshape(batched_mul(∇ϕ_Y, ∇Y_t), size(ϕ(Yt)))\n",
    "    \n",
    "    C = (α * ℯ^(-α*T)) / (1-ℯ^(-α*T))\n",
    "    l = ϕ̇ .+ α .* ϕ(Yt) .+ C .* ϕ(U_0)\n",
    "\n",
    "    isnothing(λ_pf_batch) || (l = l .* λ_pf_batch)\n",
    "    loss = relu(l .+ 1e-6)\n",
    "    return sum(loss) / size(loss)[end]\n",
    "end\n",
    "\n",
    "function get_model(name)\n",
    "    model_path = joinpath(@__DIR__, \"./model/\")\n",
    "    @assert name in readdir(model_path)\n",
    "    model_file = name\n",
    "    return BSON.load(joinpath(model_path, model_file), @__MODULE__)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbce87",
   "metadata": {},
   "source": [
    "Replace the pretrained neural operator path `NEURAL_OPERATOR_PATH` with the one saved in `train_hyper_all_pf.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80a615-18d5-4383-91cd-c9c642087d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = true\n",
    "η₀ = 1.0f-3\n",
    "λ = 1.0f-4\n",
    "total_epoch = 20\n",
    "pretrained_NO=\"NEURAL_OPERATOR_PATH\"\n",
    "if cuda && CUDA.has_cuda()\n",
    "    device = gpu\n",
    "    CUDA.allowscalar(false)\n",
    "    @info \"Training on GPU\"\n",
    "else\n",
    "    device = cpu\n",
    "    @info \"Training on CPU\"\n",
    "end\n",
    "\n",
    "lr_NO = η₀\n",
    "lr_CBF = 0.001 \n",
    "lr_CBF = 0.01\n",
    "\n",
    "lr_decay_rate = 0.2\n",
    "lr_decay_epoch =4\n",
    "\n",
    "train_loader, test_loader = my_get_dataloader()\n",
    "model_NO = FourierNeuralOperator(ch = (2, 64, 64, 64, 64, 64, 128, 1), modes = (16,), \n",
    "                              σ = gelu)\n",
    "if isnothing(pretrained_NO)\n",
    "    model_NO = FourierNeuralOperator(ch = (2, 64, 64, 64, 64, 64, 128, 1), modes = (16,), \n",
    "                              σ = gelu)\n",
    "else\n",
    "    model_NO = get_model(pretrained_NO)[:model_NO]\n",
    "end\n",
    "model_CBF = Chain(\n",
    "        Dense(2 => 16, relu),   # activation function inside layer\n",
    "        Dense(16 => 64, relu),   # activation function inside layer\n",
    "        Dense(64 => 16, relu),   # activation function inside layer\n",
    "        Dense(16 => 1)\n",
    "    )\n",
    "\n",
    "optim_NO = Flux.setup(Flux.Optimise.AdamW(η₀, (0.9, 0.999), λ), model_NO)\n",
    "optim_CBF = Flux.setup(Flux.Optimise.NADAM(lr_CBF, (0.9, 0.999), 0.1), model_CBF)\n",
    "sched_CBF = ParameterSchedulers.Stateful(Step(lr_CBF, lr_decay_rate, lr_decay_epoch)) # setup schedule of your choice\n",
    "\n",
    "\n",
    "loss_func = l₂loss\n",
    "α = 0.00001\n",
    "pf_ϵ = 0.1\n",
    "λ_pf = 1\n",
    "λ_reg = 1\n",
    "all_flag = false\n",
    "minus_safe_flag = true # cannot be false if the end can be not pf\n",
    "\n",
    "\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "no_training_losses = []\n",
    "no_test_losses = []\n",
    "least_loss = 1000\n",
    "test_loss = 0\n",
    "loss = 0\n",
    "for epoch in ProgressBar(1:total_epoch)\n",
    "    training_loss_epoch = []\n",
    "    test_loss_epoch = []\n",
    "    no_training_loss_epoch = []\n",
    "    no_test_loss_epoch = []\n",
    "    for item in train_loader\n",
    "        x_batch = item[1]\n",
    "        y_batch = item[2]\n",
    "        safe_batch = item[3]\n",
    "\n",
    "        λ_pf_batch = zeros(size(safe_batch)) \n",
    "        pf_index = findall(x->x==1, safe_batch[end, :])\n",
    "\n",
    "        size(pf_index)[1] != 0 && (λ_pf_batch[end,pf_index] .= λ_pf)\n",
    "        λ_pf_batch[:,:] .= λ_pf_batch[end:end,:]\n",
    "\n",
    "        \n",
    "        # train CBF\n",
    "        x = copy(y_batch)\n",
    "        y_init = copy(safe_batch)\n",
    "        x = vcat(x[1,:,:]...)\n",
    "        x = reshape(x, (1, size(x)[1]))\n",
    "        y_init = vcat(y_init...)\n",
    "\n",
    "        U_0 = copy(x_batch)\n",
    "        U_0[2:2,:,:] .= x_batch[2:2,1:1,:]\n",
    "        U_0 = vcat(U_0[2:2,:,:][1,:,:]...)\n",
    "        U_0 = reshape(U_0, (1, size(U_0)[1]))\n",
    "        U̇ = find_derivative(x_batch)\n",
    "        extended_U̇ = cat(ones(size(U̇)),U̇,dims=1)\n",
    "        T = x_batch[1,end,1]\n",
    "        _, ∇ϕ = Zygote.pullback(model_NO, x_batch)\n",
    "        ∇Y_t = find_derivative(cat(x_batch[1:1,:,:], y_batch, dims=1)) # empirical derivative\n",
    "\n",
    "        yt = cat(x_batch[1:1,:,:], y_batch, dims=1) # NO\n",
    "        ytt = reshape(yt, (size(yt)[1], size(yt)[2]*size(yt)[3]))\n",
    "        extended_∇Y_t = cat(ones(size(∇Y_t[1,:,:,:])),∇Y_t[1,:,:,:],dims=1) # NO\n",
    "        U_0t = cat(x_batch[1:1,:,:], reshape(U_0, size(y_batch)), dims=1) # NO\n",
    "        U_0tt = reshape(U_0t, (size(U_0t)[1], size(U_0t)[2]*size(U_0t)[3]))\n",
    "        extended_∇Y_t = reshape(extended_∇Y_t, (size(extended_∇Y_t)[1],1, size(extended_∇Y_t)[2:end]...))\n",
    "        extended_∇Y_tt = reshape(extended_∇Y_t, (size(extended_∇Y_t)[1],size(extended_∇Y_t)[2], size(extended_∇Y_t)[3]*size(extended_∇Y_t)[4]))\n",
    "        \n",
    "        CBF_training_loss, CBF_grads = Flux.withgradient(model_CBF) do m \n",
    "            loss_naive_safeset(m, ytt, y_init)  +  λ_reg .* loss_regularization(m, ytt, y_init) + λ_pf .* loss_pf(m, x_batch, ytt, U_0tt,extended_U̇, extended_∇Y_tt,T, α,y_init;all=all_flag,ϵ = pf_ϵ,λ_pf_batch=λ_pf_batch) + loss_naive_safeset_end(m, ytt, y_init;minus_safe=minus_safe_flag)  +  λ_reg .* loss_regularization_end(m, ytt, y_init;minus_safe=minus_safe_flag)\n",
    "        end\n",
    "        \n",
    "        Flux.update!(optim_CBF, model_CBF, CBF_grads[1])\n",
    "\n",
    "        loss = loss_naive_safeset(model_CBF, ytt, y_init)  +  λ_reg .* loss_regularization(model_CBF, ytt, y_init) + λ_pf .* loss_pf(model_CBF, x_batch, ytt, U_0tt,extended_U̇, extended_∇Y_tt,T, α,y_init;all=all_flag,ϵ = pf_ϵ,λ_pf_batch=λ_pf_batch) + loss_naive_safeset_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag)  +  λ_reg .* loss_regularization_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag)\n",
    "        @show loss_naive_safeset(model_CBF, ytt, y_init), loss_regularization(model_CBF, ytt, y_init), loss_pf(model_CBF, x_batch, ytt, U_0tt,extended_U̇, extended_∇Y_tt,T, α,y_init;all=all_flag,ϵ = pf_ϵ,λ_pf_batch=λ_pf_batch), loss_naive_safeset_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag), loss_regularization_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag)\n",
    "        push!(training_loss_epoch, loss)  # logging, outside gradient context\n",
    "    end\n",
    "    for item in test_loader\n",
    "        x_batch = item[1]\n",
    "        y_batch = item[2]\n",
    "        safe_batch = item[3]\n",
    "\n",
    "        λ_pf_batch = zeros(size(safe_batch)) \n",
    "        pf_index = findall(x->x==1, safe_batch[end, :])\n",
    "\n",
    "        size(pf_index)[1] != 0 && (λ_pf_batch[end,pf_index] .= λ_pf)\n",
    "        λ_pf_batch[:,:] .= λ_pf_batch[end:end,:]\n",
    "\n",
    "        x = copy(y_batch)\n",
    "        y_init = copy(safe_batch)\n",
    "        x = vcat(x[1,:,:]...)\n",
    "        x = reshape(x, (1, size(x)[1]))\n",
    "        y_init = vcat(y_init...)\n",
    "\n",
    "        U_0 = copy(x_batch)\n",
    "        U_0[2:2,:,:] .= x_batch[2:2,1:1,:]\n",
    "        U_0 = vcat(U_0[2:2,:,:][1,:,:]...)\n",
    "        U_0 = reshape(U_0, (1, size(U_0)[1]))\n",
    "        U̇ = find_derivative(x_batch)\n",
    "        extended_U̇ = cat(ones(size(U̇)),U̇,dims=1)\n",
    "        T = x_batch[1,end,1]\n",
    "        _, ∇ϕ = Zygote.pullback(model_NO, x_batch)\n",
    "        ∇Y_t = find_derivative(cat(x_batch[1:1,:,:], y_batch, dims=1)) # empirical derivative\n",
    "\n",
    "        yt = cat(x_batch[1:1,:,:], y_batch, dims=1) # NO\n",
    "        ytt = reshape(yt, (size(yt)[1], size(yt)[2]*size(yt)[3]))\n",
    "        extended_∇Y_t = cat(ones(size(∇Y_t[1,:,:,:])),∇Y_t[1,:,:,:],dims=1) # NO\n",
    "        U_0t = cat(x_batch[1:1,:,:], reshape(U_0, size(y_batch)), dims=1) # NO\n",
    "        U_0tt = reshape(U_0t, (size(U_0t)[1], size(U_0t)[2]*size(U_0t)[3]))\n",
    "        extended_∇Y_t = reshape(extended_∇Y_t, (size(extended_∇Y_t)[1],1, size(extended_∇Y_t)[2:end]...))\n",
    "        extended_∇Y_tt = reshape(extended_∇Y_t, (size(extended_∇Y_t)[1],size(extended_∇Y_t)[2], size(extended_∇Y_t)[3]*size(extended_∇Y_t)[4]))\n",
    "        \n",
    "        loss = loss_naive_safeset(model_CBF, ytt, y_init)  +  λ_reg .* loss_regularization(model_CBF, ytt, y_init) + λ_pf .* loss_pf(model_CBF, x_batch, ytt, U_0tt,extended_U̇, extended_∇Y_tt,T, α,y_init;all=all_flag,ϵ = pf_ϵ,λ_pf_batch=λ_pf_batch) + loss_naive_safeset_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag)  +  λ_reg .* loss_regularization_end(model_CBF, ytt, y_init;minus_safe=minus_safe_flag)\n",
    "        push!(test_loss_epoch, loss)  # logging, outside gradient context\n",
    "    end\n",
    "    nextlr = ParameterSchedulers.next!(sched_CBF) # advance schedule\n",
    "    Optimisers.adjust!(optim_CBF, nextlr) # update optimizer state, by default this changes the learning rate `eta`\n",
    "\n",
    "    @save \"model/hyper_model_$epoch.bson\" model_CBF\n",
    "    push!(training_losses, sum(training_loss_epoch) ./ 45000) \n",
    "    push!(test_losses, sum(test_loss_epoch) ./ 5000)\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6611b-1ffd-4d96-9680-bf141330510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show training_losses, test_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
